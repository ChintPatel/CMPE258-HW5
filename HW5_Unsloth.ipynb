{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Install\n",
    "!pip install --upgrade pip\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install trl accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2️⃣ Imports\n",
    "import torch\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 3️⃣ Config & Load\n",
    "MODEL_NAME    = \"google/gemma-2-2b\"\n",
    "max_seq_length = 512\n",
    "dtype          = torch.float16       # T4 favors fp16\n",
    "load_in_4bit   = True                # QLoRA-style 4-bit weights\n",
    "\n",
    "# This handles quantization under the hood\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name    = MODEL_NAME,\n",
    "    max_seq_length= max_seq_length,\n",
    "    dtype         = dtype,\n",
    "    load_in_4bit  = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 4️⃣ Patch in LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                     = 8,\n",
    "    target_modules       = [\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha           = 16,\n",
    "    lora_dropout         = 0.05,\n",
    "    bias                 = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state         = 42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Alpaca-cleaned\n",
    "ds_chat    = load_dataset(\"yahma/alpaca-cleaned\")[\"train\"]\n",
    "train_chat = ds_chat.shuffle(42).select(range(1000))\n",
    "eval_chat  = ds_chat.shuffle(42).select(range(100,300))\n",
    "\n",
    "# Format into a single \"text\" field\n",
    "EOS = tokenizer.eos_token\n",
    "def format_chat(ex):\n",
    "    instr = ex[\"instruction\"]\n",
    "    inp   = ex[\"input\"]\n",
    "    out   = ex[\"output\"]\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\" + instr +\n",
    "        ((\"\\n### Input:\\n\"+inp) if inp else \"\") +\n",
    "        \"\\n### Response:\\n\" + out + EOS\n",
    "    )\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# use the exact same format_chat you had, but map with batched=False\n",
    "train_chat = train_chat.map(format_chat, batched=False, remove_columns=ds_chat.column_names)\n",
    "eval_chat  = eval_chat.map (format_chat, batched=False, remove_columns=ds_chat.column_names)\n",
    "\n",
    "\n",
    "# Trainer for chat\n",
    "chat_args = TrainingArguments(\n",
    "    per_device_train_batch_size   = 2,\n",
    "    gradient_accumulation_steps   = 1,\n",
    "    num_train_epochs              = 1,\n",
    "    logging_steps                 = 50,\n",
    "    save_steps                    = 200,\n",
    "    fp16                          = True,\n",
    "    output_dir                    = \"./lora_chat\"\n",
    ")\n",
    "\n",
    "trainer_chat = SFTTrainer(\n",
    "    model               = model,\n",
    "    tokenizer           = tokenizer,\n",
    "    train_dataset       = train_chat,\n",
    "    eval_dataset        = eval_chat,\n",
    "    dataset_text_field  = \"text\",\n",
    "    max_seq_length      = max_seq_length,\n",
    "    args                = chat_args,\n",
    ")\n",
    "\n",
    "trainer_chat.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MBPP\n",
    "ds_code     = load_dataset(\"commit0/mbpp\")\n",
    "train_code  = ds_code[\"train\"].shuffle(42).select(range(300))\n",
    "eval_code   = ds_code[\"validation\"].shuffle(42).select(range(90))\n",
    "\n",
    "def format_code(ex):\n",
    "    src = ex[\"prompt\"]\n",
    "    sol = ex[\"canonical_solution\"]\n",
    "    return {\"text\": (src + \"\\n\" + sol + tokenizer.eos_token)}\n",
    "\n",
    "train_code = train_code.map(format_code, batched=False, remove_columns=ds_code[\"train\"].column_names)\n",
    "eval_code  = eval_code.map(format_code,  batched=False, remove_columns=ds_code[\"validation\"].column_names)\n",
    "\n",
    "# Trainer for code\n",
    "code_args = TrainingArguments(\n",
    "    per_device_train_batch_size   = 2,\n",
    "    gradient_accumulation_steps   = 1,\n",
    "    num_train_epochs              = 1,\n",
    "    logging_steps                 = 50,\n",
    "    save_steps                    = 200,\n",
    "    fp16                          = True,\n",
    "    output_dir                    = \"./lora_code\"\n",
    ")\n",
    "\n",
    "trainer_code = SFTTrainer(\n",
    "    model               = model,\n",
    "    tokenizer           = tokenizer,\n",
    "    train_dataset       = train_code,\n",
    "    eval_dataset        = eval_code,\n",
    "    dataset_text_field  = \"text\",\n",
    "    max_seq_length      = max_seq_length,\n",
    "    args                = code_args,\n",
    ")\n",
    "\n",
    "trainer_code.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part B\n",
    "\n",
    "# 1️⃣ Load & preprocess a French text corpus (Wikipedia)\n",
    "from datasets import load_dataset\n",
    "\n",
    "# take the first 10 k articles of French Wiki\n",
    "ds_fr = load_dataset(\"wikipedia\", \"20220301.fr\", split=\"train[:10000]\")\n",
    "\n",
    "# quick filter out any empty pages\n",
    "ds_fr = ds_fr.filter(lambda x: len(x[\"text\"].strip()) > 0)\n",
    "\n",
    "# tokenize to IDs\n",
    "def tokenize_fr(ex):\n",
    "    return tokenizer(ex[\"text\"], truncation=True, max_length=max_seq_length)\n",
    "\n",
    "ds_fr = ds_fr.map(tokenize_fr, batched=True, remove_columns=[\"title\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This gives you the original Gemma-2 backbone again, without any adapters.\n",
    "model_base, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype          = torch.float16,  # keep your 4-bit + fp16 setup\n",
    "    load_in_4bit   = True,\n",
    ")\n",
    "\n",
    "# ─── Now apply the CPT LoRA adapters to this fresh model ────────────────────\n",
    "model_cpt = FastLanguageModel.get_peft_model(\n",
    "    model_base,\n",
    "    r              = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        \"lm_head\", \"embed_tokens\",\n",
    "    ],\n",
    "    lora_alpha   = 16,\n",
    "    lora_dropout = 0.05,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Part C: Chat Templates & Multi‐Task Finetuning ──────────────────────────\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# C1. Zero‐Shot Classification\n",
    "def classify(text):\n",
    "    prompt = f\"### Instruction:\\nClassify sentiment\\n### Input:\\n{text}\\n### Response:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs, max_new_tokens=16)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(classify(\"I love this product!\"))\n",
    "\n",
    "# C2. Stateful Conversational Chat\n",
    "# ─── Simple Stateful Chat Loop (no extra imports) ───────────────────────────\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "history = []  # will hold tuples of (speaker, text)\n",
    "\n",
    "def chat_step(user_input: str, max_new_tokens=64):\n",
    "    # 1) append user\n",
    "    history.append((\"User\", user_input))\n",
    "    # 2) build the full prompt\n",
    "    prompt = \"\"\n",
    "    for speaker, text in history:\n",
    "        if speaker == \"User\":\n",
    "            prompt += f\"### User: {text}\\n\"\n",
    "        else:\n",
    "            prompt += f\"### Assistant: {text}\\n\"\n",
    "    prompt += \"### Assistant:\"  # the model completes this\n",
    "\n",
    "    # 3) tokenize + generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    # 4) extract only the newly generated portion\n",
    "    gen = out[0][ inputs.input_ids.shape[-1] : ]\n",
    "    response = tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 5) append assistant\n",
    "    history.append((\"Assistant\", response))\n",
    "    return response\n",
    "\n",
    "# ─── Example Usage ───────────────────────────────────────────────────────────\n",
    "print(chat_step(\"Hi there! How are you today?\"))\n",
    "print(chat_step(\"Can you tell me a joke?\"))\n",
    "print(chat_step(\"Thanks, that was fun. What's the weather like in Paris?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# — C3 (small): Extend GPT-2’s Context Window to 2048 —\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL = \"gpt2\"  # 124M, ~500MB on disk\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "model_small = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\")\n",
    "\n",
    "# 1) Desired new context length\n",
    "NEW_CTX = 2048\n",
    "\n",
    "# 2) Grab & repeat the positional embeddings\n",
    "# GPT-2 stores them in model.transformer.wpe\n",
    "old_pos = model_small.transformer.wpe.weight.data             # [1024, hidden]\n",
    "repeat  = NEW_CTX // old_pos.size(0)                          # =2\n",
    "model_small.transformer.wpe.weight.data = old_pos.repeat(repeat, 1)\n",
    "\n",
    "# 3) Update config\n",
    "model_small.config.n_positions = NEW_CTX\n",
    "model_small.config.n_ctx       = NEW_CTX\n",
    "\n",
    "# 4) Quick test\n",
    "prompt = \"Hello \" * 300   # ~300 tokens\n",
    "ids    = tok(prompt, return_tensors=\"pt\").input_ids.to(model_small.device)\n",
    "out    = model_small.generate(ids, max_new_tokens=20)\n",
    "print(\"✅ Extended to\", model_small.config.n_positions, \"tokens.\")\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "state": {},
   "version_major": 2,
   "version_minor": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
