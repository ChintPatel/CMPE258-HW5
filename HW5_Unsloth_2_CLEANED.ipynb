{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ Dâ€“G on facebook/opt-125m â€” setup\n",
    "!pip install --upgrade pip\n",
    "!pip install unsloth transformers accelerate bitsandbytes datasets trl\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import DPOTrainer, DPOConfig, ORPOTrainer, ORPOConfig, SFTTrainer\n",
    "\n",
    "MODEL = \"facebook/opt-125m\"\n",
    "MAX_LEN = 512\n",
    "DEVICE  = \"cuda\"\n",
    "\n",
    "# Load & quantize lightly\n",
    "base, tok = FastLanguageModel.from_pretrained(\n",
    "    model_name     = MODEL,\n",
    "    max_seq_length = MAX_LEN,\n",
    "    load_in_4bit   = True,\n",
    "    dtype          = torch.float16,\n",
    "    device_map     = \"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Part D: DPO Reward Modeling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# 1) Load your own DPO data:\n",
    "#    It must have columns: \"prompt\", \"chosen\", \"rejected\".\n",
    "#    Example JSONL format:\n",
    "#      {\"prompt\":\"Tell me a joke\",\"chosen\":\"Why did...\",\"rejected\":\"Hereâ€™s a story...\"}\n",
    "try:\n",
    "    ds_dpo = load_dataset(\"json\", data_files=\"dpo_data.jsonl\", split=\"train\")\n",
    "except Exception:\n",
    "    # fallback to a minimal dummy dataset\n",
    "    ds_dpo = Dataset.from_list([\n",
    "        {\"prompt\":\"What is 2+2?\",\"chosen\":\"4\",\"rejected\":\"5\"},\n",
    "        {\"prompt\":\"Greet me\",\"chosen\":\"Hello there!\",\"rejected\":\"Hi.\"}\n",
    "    ])\n",
    "\n",
    "# 2) Reload a clean LO-RA-patched model\n",
    "m_dpo, t_dpo = FastLanguageModel.from_pretrained(\n",
    "    MODEL, max_seq_length=MAX_LEN,\n",
    "    load_in_4bit=True, dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "m_dpo = FastLanguageModel.get_peft_model(\n",
    "    m_dpo, r=4, target_modules=[\"q_proj\",\"v_proj\"],\n",
    "    lora_alpha=8, lora_dropout=0.1, bias=\"none\"\n",
    ")\n",
    "\n",
    "# 3) DPOConfig & Trainer\n",
    "cfg = DPOConfig(\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    fp16=True,\n",
    "    output_dir=\"./dpo_opt125\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer = DPOTrainer(\n",
    "    model     = m_dpo,\n",
    "    ref_model = None,           # uses an internal copy as reference\n",
    "    train_dataset = ds_dpo,\n",
    "    tokenizer = t_dpo,\n",
    "    args      = cfg\n",
    ")\n",
    "\n",
    "# 4) Launch\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Part D (continued): ORPO Reward Modeling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import ORPOTrainer, ORPOConfig\n",
    "\n",
    "# 1) Load your ORPO data: expect columns like \"prompt\", \"chosen\", \"rejected\"\n",
    "#    or whatever format your ORPO JSONL uses.\n",
    "try:\n",
    "    ds_orpo = load_dataset(\"json\", data_files=\"orpo_data.jsonl\", split=\"train\")\n",
    "except Exception:\n",
    "    # minimal fallback so the code runs end-to-end\n",
    "    ds_orpo = Dataset.from_list([\n",
    "        {\"prompt\":\"Whatâ€™s 3+3?\",\"chosen\":\"6\",\"rejected\":\"5\"},\n",
    "        {\"prompt\":\"Say hello\",\"chosen\":\"Hello there!\",\"rejected\":\"Hi.\"},\n",
    "    ])\n",
    "\n",
    "# 2) Reload a fresh LoRA-patched model\n",
    "m_orpo, t_orpo = FastLanguageModel.from_pretrained(\n",
    "    MODEL,\n",
    "    max_seq_length = MAX_LEN,\n",
    "    load_in_4bit   = True,\n",
    "    dtype          = torch.float16,\n",
    "    device_map     = \"auto\",\n",
    ")\n",
    "m_orpo = FastLanguageModel.get_peft_model(\n",
    "    m_orpo,\n",
    "    r              = 4,\n",
    "    target_modules = [\"q_proj\",\"v_proj\"],\n",
    "    lora_alpha     = 8,\n",
    "    lora_dropout   = 0.1,\n",
    "    bias           = \"none\",\n",
    ")\n",
    "\n",
    "# 3) ORPOConfig & Trainer\n",
    "orpo_cfg = ORPOConfig(\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    fp16=True,\n",
    "    output_dir=\"./orpo_opt125\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer_orpo = ORPOTrainer(\n",
    "    model         = m_orpo,\n",
    "    tokenizer     = t_orpo,\n",
    "    train_dataset = ds_orpo,\n",
    "    args          = orpo_cfg\n",
    ")\n",
    "\n",
    "# 4) Launch\n",
    "trainer_orpo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from peft import PeftModel\n",
    "\n",
    "ckpt_dir = Path(\"lora_chat\")  # no leading dot in the string\n",
    "model_ckpt = PeftModel.from_pretrained(base, ckpt_dir)\n",
    "\n",
    "resume_args = TrainingArguments(\n",
    "    output_dir=\"./resume_opt125\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer_resume = SFTTrainer(\n",
    "    model=model_ckpt,\n",
    "    tokenizer=tok,\n",
    "    train_dataset=ds,         # e.g. reuse ds from DPO or any other\n",
    "    eval_dataset=ds.select(range(50)),\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_LEN,\n",
    "    args=resume_args,\n",
    ")\n",
    "trainer_resume.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune on a small mental-health JSONL\n",
    "ds_mh = load_dataset(\"json\", data_files=\"mental_health.jsonl\")[\"train\"].select(range(200))\n",
    "ds_mh = ds_mh.map(lambda x: {\"text\": f\"### Instruction:\\nSupport kindly\\n### Input:\\n{x['prompt']}\\n### Response:\\n{x['response']}{tok.eos_token}\"}, batched=False)\n",
    "\n",
    "m_mh = FastLanguageModel.get_peft_model(\n",
    "    base, r=4, target_modules=[\"q_proj\",\"v_proj\"], lora_alpha=8, lora_dropout=0.1, bias=\"none\"\n",
    ")\n",
    "mh_args = TrainingArguments(\n",
    "    output_dir=\"./mh_opt125\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer_mh = SFTTrainer(\n",
    "    model=m_mh, tokenizer=tok, train_dataset=ds_mh, eval_dataset=ds_mh.select(range(50)),\n",
    "    dataset_text_field=\"text\", max_seq_length=MAX_LEN, args=mh_args\n",
    ")\n",
    "trainer_mh.train()\n",
    "\n",
    "# Export to Ollama (example)\n",
    "m_mh.save_pretrained(\"mh_opt125_adapter\")\n",
    "tok.save_pretrained(\"mh_opt125_adapter\")\n",
    "# !ollama create mh_opt125 -f mh_opt125_adapter\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "state": {},
   "version_major": 2,
   "version_minor": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
